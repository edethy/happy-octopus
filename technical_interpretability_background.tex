%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Related Work}
The past few years have seen an explosian of papers from the technical community related to providing an explanation for a neural network\vtick s decision.
These methods range from manipulating features of the input to see how they related to the output, to attempting to gain a deep understanding of the internals of the network.
Work in this area includes research into developing interpretable facial recognition models.

This thesis focuses on applying a singular method of understanding the internals of a neural network to facial characterization networks. Thus, I will briefly survey three techniques focused on developing a comprehension for the internals of a deep neural network and research related to interpretable facial recognition in particular.
The three techniques I will discuss are explanation generation, saliency mapping, and representation analysis.


\subsection{Explanation Generation}
Explanation generation is a technique used to provide human-readable explanations for the decisions of a neural network. As the network is trained on the desired task, a second network is trained using textual explanations to generate an explanation. Models trained in this way perform well on the assigned task and providing a human understandable explanation. Despite this, adding a second neural network to generate the explanation increases the complexity of the overall model and it is not known if the generated explanations are correct or merely producing explanations that sound good to a human.

\subsection{Salience Mapping}
Saliency mapping is a technique that can be applied to deep neural networks for computer vision problems to identify features in an image relevant to classifying it.  The output is a heat map of input sensitivity.

For a given image, portions of the image are occluded to determine which regions affect the network's output. Salience mapping provides useful information to understand why the network made a particular classification. However, saliency mapping does not provide insight into the higher level decision making of the network such as why a network classified a cat image as a cat and a bird image as a bird. 



\subsection{Representation Analysis}
Representation analysis pertains to determining learned concepts in the hidden units of the network. The goal is to, for a given network, determine the concepts individual layers in the network learn. The technique to do this is called NetworkDissection and it is discussed in greater detail in \textbf{Section X}. I used the network dissection method to provide the interpretability of facial characterization networks. 

\subsection{Facial Recognition Interpretability}

Work is being done in this space as well.
